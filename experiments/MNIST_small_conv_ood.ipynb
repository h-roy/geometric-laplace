{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import matfree\n",
    "import tree_math as tm\n",
    "from flax import linen as nn\n",
    "from jax import nn as jnn\n",
    "from jax import numpy as jnp\n",
    "from jax import random, jit\n",
    "import pickle\n",
    "from src.losses import mse_loss, accuracy_preds, nll\n",
    "from src.helper import calculate_exact_ggn, tree_random_normal_like\n",
    "from src.sampling.predictive_samplers import sample_predictive, sample_hessian_predictive\n",
    "from jax import flatten_util\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from src.data.torch_datasets import MNIST, numpy_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 10#1000\n",
    "train_samples = 200#1000\n",
    "classes_train = [0,1,2,3,4,5,6,7,8,9]\n",
    "n_classes = 10\n",
    "batch_size = 256#20\n",
    "test_batch_size = 256\n",
    "\n",
    "data_train = MNIST(path_root= \"/work3/hroy/data/\",\n",
    "            train=True, n_samples=train_samples if train_samples > 0 else None, cls=classes_train\n",
    "        )\n",
    "data_test = MNIST(path_root = \"/work3/hroy/data/\", train=False, cls=classes_train)\n",
    "\n",
    "if train_samples > 0:\n",
    "    N = train_samples * n_classes\n",
    "else:\n",
    "    N = len(data_train)\n",
    "N_test = len(data_test)\n",
    "if test_batch_size > 0:\n",
    "    test_batch_size = test_batch_size\n",
    "else:\n",
    "    test_batch_size = len(data_test)\n",
    "\n",
    "n_test_batches = int(N_test / test_batch_size)\n",
    "n_batches = int(N / batch_size)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    data_train, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    data_test, batch_size=test_batch_size, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    output_dim: int = 10\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if len(x.shape) != 4:\n",
    "            x = jnp.expand_dims(x, 0)\n",
    "        x = jnp.transpose(x, (0, 2, 3, 1))\n",
    "        x = nn.Conv(features=4, kernel_size=(3, 3), strides=(2, 2), padding=1)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.Conv(features=4, kernel_size=(3, 3), strides=(2, 2), padding=1)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        return nn.Dense(features=self.output_dim)(x)\n",
    "\n",
    "def compute_num_params(pytree):\n",
    "    return sum(x.size if hasattr(x, \"size\") else 0 for x in jax.tree_util.tree_leaves(pytree))\n",
    "\n",
    "\n",
    "model = ConvNet()\n",
    "batch = next(iter(train_loader))\n",
    "x_init, y_init = batch[\"image\"], batch[\"label\"]\n",
    "output_dim = y_init.shape[-1]\n",
    "key, split_key = random.split(jax.random.PRNGKey(0))\n",
    "params = model.init(key, x_init)\n",
    "alpha = 1.\n",
    "optim = optax.chain(\n",
    "        optax.clip(1.),\n",
    "        getattr(optax, \"adam\")(1e-2),\n",
    "    )\n",
    "opt_state = optim.init(params)\n",
    "n_params = compute_num_params(params)\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(preds, y, rho=1.0):\n",
    "    \"\"\"\n",
    "    preds: (n_samples, n_classes) (logits)\n",
    "    y: (n_samples, n_classes) (one-hot labels)\n",
    "    \"\"\"\n",
    "    preds = preds * rho\n",
    "    preds = jax.nn.log_softmax(preds, axis=-1)\n",
    "    return -jnp.sum(jnp.sum(preds * y, axis=-1))\n",
    "\n",
    "def accuracy(params, model, batch_x, batch_y):\n",
    "    preds = model.apply(params, batch_x)\n",
    "    return jnp.sum(preds.argmax(axis=-1) == batch_y.argmax(axis=-1))\n",
    "\n",
    "\n",
    "def map_loss(\n",
    "    params,\n",
    "    model,\n",
    "    x_batch,\n",
    "    y_batch,\n",
    "    alpha,\n",
    "    n_params: int,\n",
    "    N_datapoints_max: int,\n",
    "):\n",
    "    # define dict for logging purposes\n",
    "    B = x_batch.shape[0]\n",
    "    O = y_batch.shape[-1]\n",
    "    D = n_params\n",
    "    N = N_datapoints_max\n",
    "\n",
    "    # hessian_scaler = 1\n",
    "\n",
    "    vparams = tm.Vector(params)\n",
    "\n",
    "    rho = 1.\n",
    "    nll = lambda x, y, rho: 1/B * cross_entropy_loss(x, y, rho)\n",
    "\n",
    "    y_pred = model.apply(params, x_batch)\n",
    "\n",
    "    loglike_loss = nll(y_pred, y_batch, rho) #* hessian_scaler\n",
    "\n",
    "    log_prior_term = -D / 2 * jnp.log(2 * jnp.pi) - (1 / 2) * alpha * (vparams @ vparams) + D / 2 * jnp.log(alpha)\n",
    "    # log_det_term = 0\n",
    "    loss = loglike_loss - 0. * log_prior_term\n",
    "\n",
    "    return loss\n",
    "\n",
    "def make_step(params, alpha, opt_state, x, y):\n",
    "    grad_fn = jax.value_and_grad(map_loss, argnums=0, has_aux=False)\n",
    "    loss, grads = grad_fn(params, model, x, y, alpha, n_params, N)\n",
    "    param_updates, opt_state = optim.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, param_updates)\n",
    "    return loss, params, opt_state\n",
    "\n",
    "jit_make_step = jit(make_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1, loss=16.185, , accuracy=0.15, alpha=1.00, time=1.267s\n",
      "epoch=2, loss=15.557, , accuracy=0.24, alpha=1.00, time=0.353s\n",
      "epoch=3, loss=14.873, , accuracy=0.28, alpha=1.00, time=0.220s\n",
      "epoch=4, loss=13.988, , accuracy=0.40, alpha=1.00, time=0.225s\n",
      "epoch=5, loss=12.888, , accuracy=0.50, alpha=1.00, time=0.221s\n",
      "epoch=6, loss=11.643, , accuracy=0.53, alpha=1.00, time=0.228s\n",
      "epoch=7, loss=10.573, , accuracy=0.56, alpha=1.00, time=0.222s\n",
      "epoch=8, loss=9.593, , accuracy=0.59, alpha=1.00, time=0.221s\n",
      "epoch=9, loss=8.729, , accuracy=0.64, alpha=1.00, time=0.220s\n",
      "epoch=10, loss=7.945, , accuracy=0.67, alpha=1.00, time=0.221s\n",
      "epoch=11, loss=7.375, , accuracy=0.69, alpha=1.00, time=0.221s\n",
      "epoch=12, loss=6.867, , accuracy=0.72, alpha=1.00, time=0.218s\n",
      "epoch=13, loss=6.442, , accuracy=0.73, alpha=1.00, time=0.221s\n",
      "epoch=14, loss=6.041, , accuracy=0.75, alpha=1.00, time=0.219s\n",
      "epoch=15, loss=5.740, , accuracy=0.75, alpha=1.00, time=0.218s\n",
      "epoch=16, loss=5.532, , accuracy=0.76, alpha=1.00, time=0.224s\n",
      "epoch=17, loss=5.378, , accuracy=0.77, alpha=1.00, time=0.224s\n",
      "epoch=18, loss=5.064, , accuracy=0.79, alpha=1.00, time=0.226s\n",
      "epoch=19, loss=5.006, , accuracy=0.79, alpha=1.00, time=0.218s\n",
      "epoch=20, loss=4.782, , accuracy=0.80, alpha=1.00, time=0.220s\n",
      "epoch=21, loss=4.596, , accuracy=0.82, alpha=1.00, time=0.219s\n",
      "epoch=22, loss=4.544, , accuracy=0.81, alpha=1.00, time=0.221s\n",
      "epoch=23, loss=4.472, , accuracy=0.82, alpha=1.00, time=0.221s\n",
      "epoch=24, loss=4.275, , accuracy=0.82, alpha=1.00, time=0.218s\n",
      "epoch=25, loss=4.216, , accuracy=0.82, alpha=1.00, time=0.228s\n",
      "epoch=26, loss=4.103, , accuracy=0.82, alpha=1.00, time=0.221s\n",
      "epoch=27, loss=3.992, , accuracy=0.83, alpha=1.00, time=0.222s\n",
      "epoch=28, loss=3.990, , accuracy=0.83, alpha=1.00, time=0.220s\n",
      "epoch=29, loss=3.988, , accuracy=0.83, alpha=1.00, time=0.224s\n",
      "epoch=30, loss=3.882, , accuracy=0.84, alpha=1.00, time=0.219s\n",
      "epoch=31, loss=3.759, , accuracy=0.84, alpha=1.00, time=0.230s\n",
      "epoch=32, loss=3.758, , accuracy=0.84, alpha=1.00, time=0.217s\n",
      "epoch=33, loss=3.697, , accuracy=0.84, alpha=1.00, time=0.229s\n",
      "epoch=34, loss=3.736, , accuracy=0.84, alpha=1.00, time=0.229s\n",
      "epoch=35, loss=3.547, , accuracy=0.85, alpha=1.00, time=0.222s\n",
      "epoch=36, loss=3.581, , accuracy=0.84, alpha=1.00, time=0.228s\n",
      "epoch=37, loss=3.572, , accuracy=0.85, alpha=1.00, time=0.218s\n",
      "epoch=38, loss=3.505, , accuracy=0.85, alpha=1.00, time=0.230s\n",
      "epoch=39, loss=3.467, , accuracy=0.85, alpha=1.00, time=0.222s\n",
      "epoch=40, loss=3.442, , accuracy=0.85, alpha=1.00, time=0.222s\n",
      "epoch=41, loss=3.430, , accuracy=0.85, alpha=1.00, time=0.218s\n",
      "epoch=42, loss=3.420, , accuracy=0.85, alpha=1.00, time=0.220s\n",
      "epoch=43, loss=3.286, , accuracy=0.86, alpha=1.00, time=0.220s\n",
      "epoch=44, loss=3.343, , accuracy=0.85, alpha=1.00, time=0.222s\n",
      "epoch=45, loss=3.342, , accuracy=0.86, alpha=1.00, time=0.218s\n",
      "epoch=46, loss=3.335, , accuracy=0.86, alpha=1.00, time=0.224s\n",
      "epoch=47, loss=3.335, , accuracy=0.85, alpha=1.00, time=0.213s\n",
      "epoch=48, loss=3.319, , accuracy=0.85, alpha=1.00, time=0.217s\n",
      "epoch=49, loss=3.276, , accuracy=0.85, alpha=1.00, time=0.232s\n",
      "epoch=50, loss=3.296, , accuracy=0.85, alpha=1.00, time=0.223s\n",
      "epoch=51, loss=3.246, , accuracy=0.86, alpha=1.00, time=0.220s\n",
      "epoch=52, loss=3.227, , accuracy=0.86, alpha=1.00, time=0.228s\n",
      "epoch=53, loss=3.216, , accuracy=0.86, alpha=1.00, time=0.212s\n",
      "epoch=54, loss=3.146, , accuracy=0.86, alpha=1.00, time=0.224s\n",
      "epoch=55, loss=3.129, , accuracy=0.86, alpha=1.00, time=0.220s\n",
      "epoch=56, loss=3.110, , accuracy=0.86, alpha=1.00, time=0.225s\n",
      "epoch=57, loss=3.149, , accuracy=0.86, alpha=1.00, time=0.220s\n",
      "epoch=58, loss=3.171, , accuracy=0.86, alpha=1.00, time=0.219s\n",
      "epoch=59, loss=3.053, , accuracy=0.87, alpha=1.00, time=0.224s\n",
      "epoch=60, loss=2.987, , accuracy=0.87, alpha=1.00, time=0.219s\n",
      "epoch=61, loss=2.977, , accuracy=0.87, alpha=1.00, time=0.220s\n",
      "epoch=62, loss=3.032, , accuracy=0.86, alpha=1.00, time=0.220s\n",
      "epoch=63, loss=3.084, , accuracy=0.87, alpha=1.00, time=0.227s\n",
      "epoch=64, loss=2.966, , accuracy=0.87, alpha=1.00, time=0.220s\n",
      "epoch=65, loss=2.988, , accuracy=0.87, alpha=1.00, time=0.218s\n",
      "epoch=66, loss=3.055, , accuracy=0.86, alpha=1.00, time=0.223s\n",
      "epoch=67, loss=2.914, , accuracy=0.87, alpha=1.00, time=0.220s\n",
      "epoch=68, loss=2.908, , accuracy=0.87, alpha=1.00, time=0.224s\n",
      "epoch=69, loss=2.950, , accuracy=0.87, alpha=1.00, time=0.221s\n",
      "epoch=70, loss=2.940, , accuracy=0.87, alpha=1.00, time=0.224s\n",
      "epoch=71, loss=2.994, , accuracy=0.87, alpha=1.00, time=0.222s\n",
      "epoch=72, loss=2.949, , accuracy=0.87, alpha=1.00, time=0.221s\n",
      "epoch=73, loss=2.961, , accuracy=0.86, alpha=1.00, time=0.222s\n",
      "epoch=74, loss=2.883, , accuracy=0.87, alpha=1.00, time=0.217s\n",
      "epoch=75, loss=2.905, , accuracy=0.87, alpha=1.00, time=0.222s\n",
      "epoch=76, loss=2.908, , accuracy=0.86, alpha=1.00, time=0.225s\n",
      "epoch=77, loss=2.991, , accuracy=0.86, alpha=1.00, time=0.222s\n",
      "epoch=78, loss=2.862, , accuracy=0.87, alpha=1.00, time=0.220s\n",
      "epoch=79, loss=2.893, , accuracy=0.87, alpha=1.00, time=0.218s\n",
      "epoch=80, loss=2.895, , accuracy=0.86, alpha=1.00, time=0.220s\n",
      "epoch=81, loss=2.934, , accuracy=0.87, alpha=1.00, time=0.233s\n",
      "epoch=82, loss=2.896, , accuracy=0.87, alpha=1.00, time=0.217s\n",
      "epoch=83, loss=2.863, , accuracy=0.87, alpha=1.00, time=0.213s\n",
      "epoch=84, loss=2.771, , accuracy=0.88, alpha=1.00, time=0.220s\n",
      "epoch=85, loss=2.847, , accuracy=0.87, alpha=1.00, time=0.222s\n",
      "epoch=86, loss=2.880, , accuracy=0.86, alpha=1.00, time=0.227s\n",
      "epoch=87, loss=2.847, , accuracy=0.87, alpha=1.00, time=0.218s\n",
      "epoch=88, loss=2.868, , accuracy=0.87, alpha=1.00, time=0.236s\n",
      "epoch=89, loss=2.827, , accuracy=0.87, alpha=1.00, time=0.215s\n",
      "epoch=90, loss=2.841, , accuracy=0.87, alpha=1.00, time=0.216s\n",
      "epoch=91, loss=2.701, , accuracy=0.87, alpha=1.00, time=0.220s\n",
      "epoch=92, loss=2.827, , accuracy=0.87, alpha=1.00, time=0.218s\n",
      "epoch=93, loss=2.822, , accuracy=0.87, alpha=1.00, time=0.220s\n",
      "epoch=94, loss=2.754, , accuracy=0.87, alpha=1.00, time=0.224s\n",
      "epoch=95, loss=2.813, , accuracy=0.87, alpha=1.00, time=0.222s\n",
      "epoch=96, loss=2.738, , accuracy=0.87, alpha=1.00, time=0.216s\n",
      "epoch=97, loss=2.838, , accuracy=0.87, alpha=1.00, time=0.219s\n",
      "epoch=98, loss=2.820, , accuracy=0.87, alpha=1.00, time=0.226s\n",
      "epoch=99, loss=2.771, , accuracy=0.87, alpha=1.00, time=0.217s\n",
      "epoch=100, loss=2.836, , accuracy=0.87, alpha=1.00, time=0.228s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    start_time = time.time()\n",
    "    for _, batch in zip(range(n_batches), train_loader):\n",
    "        X = batch[\"image\"]\n",
    "        y = batch[\"label\"]\n",
    "        B = X.shape[0]\n",
    "        train_key, split_key = random.split(split_key)\n",
    "\n",
    "        loss, params, opt_state = jit_make_step(params, alpha, opt_state, X, y)\n",
    "        loss = loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        epoch_accuracy += accuracy(params, model, X, y).item()\n",
    "\n",
    "    epoch_accuracy /= (n_batches * B)\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(\n",
    "        f\"epoch={epoch}, loss={epoch_loss:.3f}, , accuracy={epoch_accuracy:.2f}, alpha={alpha:.2f}, time={epoch_time:.3f}s\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_train_loader = torch.utils.data.DataLoader(\n",
    "    data_train, batch_size=N, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
    ")\n",
    "data = next(iter(sampling_train_loader))\n",
    "x_train = jnp.array(data[\"image\"])\n",
    "y_train = jnp.array(data[\"label\"])\n",
    "sampling_val_loader = torch.utils.data.DataLoader(\n",
    "    data_test, batch_size=N_test, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
    ")\n",
    "data = next(iter(sampling_val_loader))\n",
    "x_val = jnp.array(data[\"image\"])\n",
    "y_val = jnp.array(data[\"label\"])\n",
    "\n",
    "sample_key = jax.random.PRNGKey(0)\n",
    "n_posterior_samples = 200\n",
    "num_iterations = 1\n",
    "n_sample_batch_size = 1\n",
    "n_sample_batches = N // n_sample_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sampling.exact_ggn import exact_ggn_laplace\n",
    "from src.sampling.laplace_ode import ode_ggn\n",
    "from src.sampling.lanczos_diffusion import lanczos_diffusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_fn = lambda params, x: model.apply(params, x[None, ...])[0]\n",
    "ggn = calculate_exact_ggn(cross_entropy_loss, _model_fn, params, x_train, y_train, n_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(337, dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.linalg.matrix_rank(ggn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_key = jax.random.PRNGKey(0)\n",
    "model_fn = lambda params, x: model.apply(params, x[None, ...])[0]\n",
    "rank = 10\n",
    "alpha = 1.0\n",
    "n_posterior_samples = 200\n",
    "var = 0.1\n",
    "\n",
    "lr_posterior_samples, posterior_samples, isotropic_posterior_samples = exact_ggn_laplace(cross_entropy_loss, \n",
    "                                                                                         model_fn,\n",
    "                                                                                         params,\n",
    "                                                                                         x_train,\n",
    "                                                                                         y_train,\n",
    "                                                                                         n_params,\n",
    "                                                                                         rank,\n",
    "                                                                                         alpha,\n",
    "                                                                                         n_posterior_samples,\n",
    "                                                                                         sample_key,\n",
    "                                                                                         var,\n",
    "                                                                                         \"all\"\n",
    "                                                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_val = 1000\n",
    "lr_predictive = sample_predictive(lr_posterior_samples, params, model, x_val[:N_val], False, \"Pytree\")\n",
    "predictive = sample_predictive(posterior_samples, params, model, x_val[:N_val], False, \"Pytree\")\n",
    "isotropic_predictive = sample_predictive(isotropic_posterior_samples, params, model, x_val[:N_val], False, \"Pytree\")\n",
    "preds = model.apply(params, x_val[:N_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP acc: 0.843\n",
      "lr acc: 0.84208494\n",
      "full acc: 0.79846\n",
      "isotropic acc: 0.77830505\n"
     ]
    }
   ],
   "source": [
    "print(\"MAP acc:\", accuracy(params, model, x_val[:N_val], y_val[:N_val])/x_val[:N_val].shape[0])\n",
    "print(\"lr acc:\", jnp.mean(jax.vmap(accuracy_preds, in_axes=(0,None))(lr_predictive, y_val[:N_val])/x_val[:N_val].shape[0]))\n",
    "print(\"full acc:\", jnp.mean(jax.vmap(accuracy_preds, in_axes=(0,None))(predictive, y_val[:N_val])/x_val[:N_val].shape[0]))\n",
    "print(\"isotropic acc:\", jnp.mean(jax.vmap(accuracy_preds, in_axes=(0,None))(isotropic_predictive, y_val[:N_val])/x_val[:N_val].shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP nll: 537.5515\n",
      "lr nll: 539.63477\n",
      "full nll: 656.6381\n",
      "isotropic nll: 711.3513\n"
     ]
    }
   ],
   "source": [
    "print(\"MAP nll:\", nll(preds, y_val[:N_val]))\n",
    "print(\"lr nll:\", nll(lr_predictive, y_val[:N_val]))\n",
    "print(\"full nll:\", nll(predictive, y_val[:N_val]))\n",
    "print(\"isotropic nll:\", nll(isotropic_predictive, y_val[:N_val]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictive = sample_predictive(lr_posterior_samples, params, model, x_val[:N_val], True, \"Pytree\")\n",
    "predictive = sample_predictive(posterior_samples, params, model, x_val[:N_val], True, \"Pytree\")\n",
    "isotropic_predictive = sample_predictive(isotropic_posterior_samples, params, model, x_val[:N_val], True, \"Pytree\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1000, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isotropic_predictive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP acc: 0.843\n",
      "lr acc: 0.842125\n",
      "full acc: 0.812935\n",
      "isotropic acc: 0.77391493\n"
     ]
    }
   ],
   "source": [
    "print(\"MAP acc:\", accuracy(params, model, x_val[:N_val], y_val[:N_val])/x_val[:N_val].shape[0])\n",
    "print(\"lr acc:\", jnp.mean(jax.vmap(accuracy_preds, in_axes=(0,None))(lr_predictive, y_val[:N_val])/x_val[:N_val].shape[0]))\n",
    "print(\"full acc:\", jnp.mean(jax.vmap(accuracy_preds, in_axes=(0,None))(predictive, y_val[:N_val])/x_val[:N_val].shape[0]))\n",
    "print(\"isotropic acc:\", jnp.mean(jax.vmap(accuracy_preds, in_axes=(0,None))(isotropic_predictive, y_val[:N_val])/x_val[:N_val].shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP nll: 537.5515\n",
      "lr nll: 540.08307\n",
      "full nll: 624.62225\n",
      "isotropic nll: 747.6211\n"
     ]
    }
   ],
   "source": [
    "print(\"MAP nll:\", nll(preds, y_val[:N_val]))\n",
    "print(\"lr nll:\", nll(lr_predictive, y_val[:N_val]))\n",
    "print(\"full nll:\", nll(predictive, y_val[:N_val]))\n",
    "print(\"isotropic nll:\", nll(isotropic_predictive, y_val[:N_val]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets import (\n",
    "    get_rotated_mnist_loaders,\n",
    "    load_corrupted_cifar10,\n",
    "    get_mnist_ood_loaders,\n",
    "    get_cifar10_ood_loaders,\n",
    "    get_cifar10_train_set,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_args = {}\n",
    "eval_args[\"linearised_laplace\"] = False\n",
    "eval_args[\"posterior_sample_type\"] = \"Pytree\"\n",
    "eval_args[\"likelihood\"] = \"classification\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-MNIST with distribution shift intensity 0\n",
      "conf: 0.8293, nll: 297.9814, acc: 0.8140, brier: 0.2763, ece: 0.1219, mce: 0.7098\n",
      "R-MNIST with distribution shift intensity 1\n",
      "conf: 0.7904, nll: 402.6505, acc: 0.7320, brier: 0.3799, ece: 0.1402, mce: 0.6985\n",
      "R-MNIST with distribution shift intensity 2\n",
      "conf: 0.7345, nll: 886.8191, acc: 0.4980, brier: 0.7143, ece: 0.2762, mce: 0.7698\n",
      "R-MNIST with distribution shift intensity 3\n",
      "conf: 0.6773, nll: 1994.0592, acc: 0.1980, brier: 1.1895, ece: 0.4839, mce: 0.9056\n",
      "R-MNIST with distribution shift intensity 4\n",
      "conf: 0.6894, nll: 2437.9873, acc: 0.1620, brier: 1.2652, ece: 0.5363, mce: 0.9752\n",
      "R-MNIST with distribution shift intensity 5\n",
      "conf: 0.6485, nll: 2278.5176, acc: 0.1620, brier: 1.2186, ece: 0.4859, mce: 0.8357\n",
      "R-MNIST with distribution shift intensity 6\n",
      "conf: 0.6566, nll: 2078.4587, acc: 0.1760, brier: 1.1976, ece: 0.4802, mce: 0.9545\n",
      "R-MNIST with distribution shift intensity 7\n",
      "conf: 0.7467, nll: 2250.4639, acc: 0.2840, brier: 1.1244, ece: 0.4850, mce: 0.8760\n",
      "R-MNIST with distribution shift intensity 8\n",
      "conf: 0.7026, nll: 2193.2632, acc: 0.2060, brier: 1.1950, ece: 0.5045, mce: 0.8464\n",
      "R-MNIST with distribution shift intensity 9\n",
      "conf: 0.6688, nll: 2475.7488, acc: 0.0940, brier: 1.3525, ece: 0.5679, mce: 0.9546\n",
      "R-MNIST with distribution shift intensity 10\n",
      "conf: 0.6455, nll: 2425.1375, acc: 0.1180, brier: 1.2952, ece: 0.5245, mce: 0.9265\n",
      "R-MNIST with distribution shift intensity 11\n",
      "conf: 0.6277, nll: 2180.3149, acc: 0.1460, brier: 1.2035, ece: 0.4833, mce: 0.9335\n",
      "R-MNIST with distribution shift intensity 12\n",
      "conf: 0.6645, nll: 997.9722, acc: 0.4720, brier: 0.7187, ece: 0.2311, mce: 0.7249\n",
      "R-MNIST with distribution shift intensity 13\n",
      "conf: 0.7573, nll: 474.7219, acc: 0.6980, brier: 0.4231, ece: 0.1392, mce: 0.6969\n",
      "R-MNIST with distribution shift intensity 14\n",
      "conf: 0.8293, nll: 297.9814, acc: 0.8140, brier: 0.2763, ece: 0.1219, mce: 0.7098\n"
     ]
    }
   ],
   "source": [
    "from src.ood_functions.evaluate import evaluate\n",
    "from src.ood_functions.metrics import compute_metrics\n",
    "ids = [0, 15, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 345, 360]\n",
    "n_datapoint=500\n",
    "ood_batch_size = 50\n",
    "metrics = []\n",
    "for i, id in enumerate(ids):    \n",
    "    _, test_loader = get_rotated_mnist_loaders(id, data_path=\"data\", download=True, batch_size=ood_batch_size, n_datapoint=n_datapoint)\n",
    "    some_metrics, all_y_prob, all_y_true, all_y_var = evaluate(test_loader, lr_posterior_samples, params, model, eval_args)\n",
    "    if i == 0:\n",
    "        all_y_prob_in = all_y_prob\n",
    "    more_metrics = compute_metrics(\n",
    "            i, id, all_y_prob, test_loader, all_y_prob_in, all_y_var, benchmark=\"R-MNIST\"\n",
    "        )\n",
    "    metrics.append({**some_metrics, **more_metrics})\n",
    "    print(\", \".join([f\"{k}: {v:.4f}\" for k, v in metrics[-1].items()]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-MNIST with distribution shift intensity 0\n",
      "conf: 0.8126, nll: 286.0074, acc: 0.8160, brier: 0.2681, ece: 0.1080, mce: 0.7015\n",
      "R-MNIST with distribution shift intensity 1\n",
      "conf: 0.7734, nll: 385.7637, acc: 0.7460, brier: 0.3662, ece: 0.1366, mce: 0.8287\n",
      "R-MNIST with distribution shift intensity 2\n",
      "conf: 0.7114, nll: 842.7404, acc: 0.5000, brier: 0.7018, ece: 0.2386, mce: 0.6550\n",
      "R-MNIST with distribution shift intensity 3\n",
      "conf: 0.6617, nll: 1906.2975, acc: 0.1900, brier: 1.1863, ece: 0.4762, mce: 0.8752\n",
      "R-MNIST with distribution shift intensity 4\n",
      "conf: 0.6748, nll: 2343.8108, acc: 0.1620, brier: 1.2489, ece: 0.5192, mce: 0.8548\n",
      "R-MNIST with distribution shift intensity 5\n",
      "conf: 0.6342, nll: 2185.4668, acc: 0.1640, brier: 1.1982, ece: 0.4690, mce: 0.9887\n",
      "R-MNIST with distribution shift intensity 6\n",
      "conf: 0.6332, nll: 1971.8813, acc: 0.1800, brier: 1.1739, ece: 0.4579, mce: 0.9133\n",
      "R-MNIST with distribution shift intensity 7\n",
      "conf: 0.7272, nll: 2125.1313, acc: 0.2820, brier: 1.1018, ece: 0.4448, mce: 0.8240\n",
      "R-MNIST with distribution shift intensity 8\n",
      "conf: 0.6844, nll: 2088.2256, acc: 0.1980, brier: 1.1944, ece: 0.5016, mce: 0.8848\n",
      "R-MNIST with distribution shift intensity 9\n",
      "conf: 0.6539, nll: 2327.7085, acc: 0.0980, brier: 1.3283, ece: 0.5571, mce: 0.9158\n",
      "R-MNIST with distribution shift intensity 10\n",
      "conf: 0.6328, nll: 2302.7290, acc: 0.1320, brier: 1.2758, ece: 0.5190, mce: 0.9379\n",
      "R-MNIST with distribution shift intensity 11\n",
      "conf: 0.6141, nll: 2076.3010, acc: 0.1600, brier: 1.1770, ece: 0.4531, mce: 0.9720\n",
      "R-MNIST with distribution shift intensity 12\n",
      "conf: 0.6422, nll: 972.1418, acc: 0.4580, brier: 0.7149, ece: 0.2090, mce: 0.6534\n",
      "R-MNIST with distribution shift intensity 13\n",
      "conf: 0.7300, nll: 461.4109, acc: 0.6920, brier: 0.4180, ece: 0.1220, mce: 0.7707\n",
      "R-MNIST with distribution shift intensity 14\n",
      "conf: 0.8126, nll: 286.0074, acc: 0.8160, brier: 0.2681, ece: 0.1080, mce: 0.7015\n"
     ]
    }
   ],
   "source": [
    "from src.ood_functions.evaluate import evaluate\n",
    "# from src.ood_functions.metrics import compute_metrics\n",
    "ids = [0, 15, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 345, 360]\n",
    "n_datapoint=500\n",
    "ood_batch_size = 50\n",
    "metrics = []\n",
    "for i, id in enumerate(ids):    \n",
    "    _, test_loader = get_rotated_mnist_loaders(id, data_path=\"data\", download=True, batch_size=ood_batch_size, n_datapoint=n_datapoint)\n",
    "    some_metrics, all_y_prob, all_y_true, all_y_var = evaluate(test_loader, posterior_samples, params, model, eval_args)\n",
    "    if i == 0:\n",
    "        all_y_prob_in = all_y_prob\n",
    "    more_metrics = compute_metrics(\n",
    "            i, id, all_y_prob, test_loader, all_y_prob_in, all_y_var, benchmark=\"R-MNIST\"\n",
    "        )\n",
    "    metrics.append({**some_metrics, **more_metrics})\n",
    "    print(\", \".join([f\"{k}: {v:.4f}\" for k, v in metrics[-1].items()]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lanczos diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2\n",
    "n_samples = 50\n",
    "alpha = 10.0\n",
    "rank = 10\n",
    "nonker_posterior_samples = lanczos_diffusion(cross_entropy_loss, model.apply, params,n_steps,n_samples,alpha,sample_key,n_params,rank,x_train,y_train,1.0,\"non-kernel-eigvals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonker_predictive = sample_predictive(nonker_posterior_samples, params, model, x_val[:N_val], False, \"Pytree\")\n",
    "preds = model.apply(params, x_val[:N_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP acc: 0.832\n",
      "nonker acc: 0.83283997\n",
      "MAP nll: 520.5774\n",
      "nonker nll: 522.54913\n"
     ]
    }
   ],
   "source": [
    "print(\"MAP acc:\", accuracy(params, model, x_val[:N_val], y_val[:N_val])/x_val[:N_val].shape[0])\n",
    "print(\"nonker acc:\", jnp.mean(jax.vmap(accuracy_preds, in_axes=(0,None))(nonker_predictive, y_val[:N_val])/x_val[:N_val].shape[0]))\n",
    "print(\"MAP nll:\", nll(preds, y_val[:N_val]))\n",
    "print(\"nonker nll:\", nll(nonker_predictive, y_val[:N_val]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
