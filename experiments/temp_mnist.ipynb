{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import matfree\n",
    "import tree_math as tm\n",
    "from flax import linen as nn\n",
    "from jax import nn as jnn\n",
    "from jax import numpy as jnp\n",
    "from jax import random, jit\n",
    "import pickle\n",
    "from src.losses import mse_loss\n",
    "from src.helper import calculate_exact_ggn, tree_random_normal_like\n",
    "from src.sampling.predictive_samplers import sample_predictive, sample_hessian_predictive\n",
    "from jax import flatten_util\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.torch_datasets import MNIST, numpy_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 10#1000\n",
    "classes_train = [0,1,2,3,4,5,6,7,8,9]\n",
    "n_classes = 10\n",
    "batch_size = 20#256\n",
    "test_batch_size = 256\n",
    "\n",
    "data_train = MNIST(path_root= \"/work3/hroy/data/\",\n",
    "            train=True, n_samples=train_samples if train_samples > 0 else None, cls=classes_train\n",
    "        )\n",
    "data_test = MNIST(path_root = \"/work3/hroy/data/\", train=False, cls=classes_train)\n",
    "\n",
    "if train_samples > 0:\n",
    "    N = train_samples * n_classes\n",
    "else:\n",
    "    N = len(data_train)\n",
    "N_test = len(data_test)\n",
    "if test_batch_size > 0:\n",
    "    test_batch_size = test_batch_size\n",
    "else:\n",
    "    test_batch_size = len(data_test)\n",
    "\n",
    "n_test_batches = int(N_test / test_batch_size)\n",
    "n_batches = int(N / batch_size)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    data_train, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    data_test, batch_size=test_batch_size, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    output_dim: int = 10\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if len(x.shape) != 4:\n",
    "            x = jnp.expand_dims(x, 0)\n",
    "        x = jnp.transpose(x, (0, 2, 3, 1))\n",
    "        x = nn.Conv(features=4, kernel_size=(3, 3), strides=(2, 2), padding=1)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.Conv(features=4, kernel_size=(3, 3), strides=(2, 2), padding=1)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        return nn.Dense(features=self.output_dim)(x)\n",
    "\n",
    "def compute_num_params(pytree):\n",
    "    return sum(x.size if hasattr(x, \"size\") else 0 for x in jax.tree_util.tree_leaves(pytree))\n",
    "\n",
    "\n",
    "model = ConvNet()\n",
    "batch = next(iter(train_loader))\n",
    "x_init, y_init = batch[\"image\"], batch[\"label\"]\n",
    "output_dim = y_init.shape[-1]\n",
    "key, split_key = random.split(jax.random.PRNGKey(0))\n",
    "params = model.init(key, x_init)\n",
    "alpha = 1.\n",
    "optim = optax.chain(\n",
    "        optax.clip(1.),\n",
    "        getattr(optax, \"adam\")(1e-2),\n",
    "    )\n",
    "opt_state = optim.init(params)\n",
    "n_params = compute_num_params(params)\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(preds, y, rho=1.0):\n",
    "    \"\"\"\n",
    "    preds: (n_samples, n_classes) (logits)\n",
    "    y: (n_samples, n_classes) (one-hot labels)\n",
    "    \"\"\"\n",
    "    preds = preds * rho\n",
    "    preds = jax.nn.log_softmax(preds, axis=-1)\n",
    "    return -jnp.sum(jnp.sum(preds * y, axis=-1))\n",
    "\n",
    "def accuracy(params, model, batch_x, batch_y):\n",
    "    preds = model.apply(params, batch_x)\n",
    "    return jnp.sum(preds.argmax(axis=-1) == batch_y.argmax(axis=-1))\n",
    "\n",
    "\n",
    "def map_loss(\n",
    "    params,\n",
    "    model,\n",
    "    x_batch,\n",
    "    y_batch,\n",
    "    alpha,\n",
    "    n_params: int,\n",
    "    N_datapoints_max: int,\n",
    "):\n",
    "    # define dict for logging purposes\n",
    "    B = x_batch.shape[0]\n",
    "    O = y_batch.shape[-1]\n",
    "    D = n_params\n",
    "    N = N_datapoints_max\n",
    "\n",
    "    # hessian_scaler = 1\n",
    "\n",
    "    vparams = tm.Vector(params)\n",
    "\n",
    "    rho = 1.\n",
    "    nll = lambda x, y, rho: 1/B * cross_entropy_loss(x, y, rho)\n",
    "\n",
    "    y_pred = model.apply(params, x_batch)\n",
    "\n",
    "    loglike_loss = nll(y_pred, y_batch, rho) #* hessian_scaler\n",
    "\n",
    "    log_prior_term = -D / 2 * jnp.log(2 * jnp.pi) - (1 / 2) * alpha * (vparams @ vparams) + D / 2 * jnp.log(alpha)\n",
    "    # log_det_term = 0\n",
    "    loss = loglike_loss - 0. * log_prior_term\n",
    "\n",
    "    return loss\n",
    "\n",
    "def make_step(params, alpha, opt_state, x, y):\n",
    "    grad_fn = jax.value_and_grad(map_loss, argnums=0, has_aux=False)\n",
    "    loss, grads = grad_fn(params, model, x, y, alpha, n_params, N)\n",
    "    param_updates, opt_state = optim.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, param_updates)\n",
    "    return loss, params, opt_state\n",
    "\n",
    "jit_make_step = jit(make_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1, loss=11.851, , accuracy=0.11, alpha=1.00, time=1.083s\n",
      "epoch=2, loss=11.520, , accuracy=0.12, alpha=1.00, time=0.082s\n",
      "epoch=3, loss=11.390, , accuracy=0.13, alpha=1.00, time=0.081s\n",
      "epoch=4, loss=11.160, , accuracy=0.20, alpha=1.00, time=0.082s\n",
      "epoch=5, loss=10.913, , accuracy=0.28, alpha=1.00, time=0.081s\n",
      "epoch=6, loss=10.590, , accuracy=0.32, alpha=1.00, time=0.081s\n",
      "epoch=7, loss=10.243, , accuracy=0.42, alpha=1.00, time=0.081s\n",
      "epoch=8, loss=9.815, , accuracy=0.47, alpha=1.00, time=0.081s\n",
      "epoch=9, loss=9.301, , accuracy=0.53, alpha=1.00, time=0.080s\n",
      "epoch=10, loss=8.806, , accuracy=0.54, alpha=1.00, time=0.081s\n",
      "epoch=11, loss=8.280, , accuracy=0.60, alpha=1.00, time=0.081s\n",
      "epoch=12, loss=7.775, , accuracy=0.66, alpha=1.00, time=0.080s\n",
      "epoch=13, loss=7.297, , accuracy=0.67, alpha=1.00, time=0.081s\n",
      "epoch=14, loss=6.859, , accuracy=0.67, alpha=1.00, time=0.081s\n",
      "epoch=15, loss=6.474, , accuracy=0.70, alpha=1.00, time=0.080s\n",
      "epoch=16, loss=6.082, , accuracy=0.71, alpha=1.00, time=0.081s\n",
      "epoch=17, loss=5.702, , accuracy=0.73, alpha=1.00, time=0.080s\n",
      "epoch=18, loss=5.369, , accuracy=0.75, alpha=1.00, time=0.080s\n",
      "epoch=19, loss=5.060, , accuracy=0.75, alpha=1.00, time=0.081s\n",
      "epoch=20, loss=4.797, , accuracy=0.75, alpha=1.00, time=0.080s\n",
      "epoch=21, loss=4.532, , accuracy=0.79, alpha=1.00, time=0.080s\n",
      "epoch=22, loss=4.269, , accuracy=0.81, alpha=1.00, time=0.081s\n",
      "epoch=23, loss=4.072, , accuracy=0.83, alpha=1.00, time=0.080s\n",
      "epoch=24, loss=3.865, , accuracy=0.83, alpha=1.00, time=0.080s\n",
      "epoch=25, loss=3.677, , accuracy=0.85, alpha=1.00, time=0.081s\n",
      "epoch=26, loss=3.508, , accuracy=0.88, alpha=1.00, time=0.087s\n",
      "epoch=27, loss=3.312, , accuracy=0.88, alpha=1.00, time=0.080s\n",
      "epoch=28, loss=3.226, , accuracy=0.87, alpha=1.00, time=0.080s\n",
      "epoch=29, loss=3.028, , accuracy=0.88, alpha=1.00, time=0.080s\n",
      "epoch=30, loss=2.953, , accuracy=0.89, alpha=1.00, time=0.080s\n",
      "epoch=31, loss=2.797, , accuracy=0.90, alpha=1.00, time=0.081s\n",
      "epoch=32, loss=2.675, , accuracy=0.90, alpha=1.00, time=0.081s\n",
      "epoch=33, loss=2.532, , accuracy=0.90, alpha=1.00, time=0.080s\n",
      "epoch=34, loss=2.412, , accuracy=0.91, alpha=1.00, time=0.081s\n",
      "epoch=35, loss=2.303, , accuracy=0.93, alpha=1.00, time=0.080s\n",
      "epoch=36, loss=2.206, , accuracy=0.94, alpha=1.00, time=0.080s\n",
      "epoch=37, loss=2.114, , accuracy=0.94, alpha=1.00, time=0.081s\n",
      "epoch=38, loss=2.014, , accuracy=0.95, alpha=1.00, time=0.080s\n",
      "epoch=39, loss=1.937, , accuracy=0.95, alpha=1.00, time=0.080s\n",
      "epoch=40, loss=1.841, , accuracy=0.96, alpha=1.00, time=0.081s\n",
      "epoch=41, loss=1.785, , accuracy=0.96, alpha=1.00, time=0.080s\n",
      "epoch=42, loss=1.694, , accuracy=0.96, alpha=1.00, time=0.079s\n",
      "epoch=43, loss=1.606, , accuracy=0.96, alpha=1.00, time=0.081s\n",
      "epoch=44, loss=1.545, , accuracy=0.97, alpha=1.00, time=0.080s\n",
      "epoch=45, loss=1.482, , accuracy=0.98, alpha=1.00, time=0.080s\n",
      "epoch=46, loss=1.421, , accuracy=0.99, alpha=1.00, time=0.081s\n",
      "epoch=47, loss=1.352, , accuracy=0.99, alpha=1.00, time=0.080s\n",
      "epoch=48, loss=1.305, , accuracy=0.99, alpha=1.00, time=0.080s\n",
      "epoch=49, loss=1.257, , accuracy=1.00, alpha=1.00, time=0.082s\n",
      "epoch=50, loss=1.205, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=51, loss=1.159, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=52, loss=1.109, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=53, loss=1.064, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=54, loss=1.024, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=55, loss=0.983, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=56, loss=0.949, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=57, loss=0.913, , accuracy=1.00, alpha=1.00, time=0.082s\n",
      "epoch=58, loss=0.879, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=59, loss=0.843, , accuracy=1.00, alpha=1.00, time=0.087s\n",
      "epoch=60, loss=0.811, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=61, loss=0.785, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=62, loss=0.753, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=63, loss=0.726, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=64, loss=0.698, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=65, loss=0.672, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=66, loss=0.648, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=67, loss=0.627, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=68, loss=0.613, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=69, loss=0.590, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=70, loss=0.566, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=71, loss=0.553, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=72, loss=0.528, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=73, loss=0.506, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=74, loss=0.492, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=75, loss=0.480, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=76, loss=0.462, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=77, loss=0.452, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=78, loss=0.441, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=79, loss=0.422, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=80, loss=0.407, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=81, loss=0.394, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=82, loss=0.384, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=83, loss=0.371, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=84, loss=0.361, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=85, loss=0.351, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=86, loss=0.341, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=87, loss=0.332, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=88, loss=0.324, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=89, loss=0.315, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=90, loss=0.306, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=91, loss=0.298, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=92, loss=0.290, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=93, loss=0.283, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=94, loss=0.277, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=95, loss=0.269, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=96, loss=0.264, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=97, loss=0.257, , accuracy=1.00, alpha=1.00, time=0.081s\n",
      "epoch=98, loss=0.251, , accuracy=1.00, alpha=1.00, time=0.080s\n",
      "epoch=99, loss=0.246, , accuracy=1.00, alpha=1.00, time=0.079s\n",
      "epoch=100, loss=0.238, , accuracy=1.00, alpha=1.00, time=0.081s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    start_time = time.time()\n",
    "    for _, batch in zip(range(n_batches), train_loader):\n",
    "        X = batch[\"image\"]\n",
    "        y = batch[\"label\"]\n",
    "        B = X.shape[0]\n",
    "        train_key, split_key = random.split(split_key)\n",
    "\n",
    "        loss, params, opt_state = jit_make_step(params, alpha, opt_state, X, y)\n",
    "        loss = loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        epoch_accuracy += accuracy(params, model, X, y).item()\n",
    "\n",
    "    epoch_accuracy /= (n_batches * B)\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(\n",
    "        f\"epoch={epoch}, loss={epoch_loss:.3f}, , accuracy={epoch_accuracy:.2f}, alpha={alpha:.2f}, time={epoch_time:.3f}s\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_train_loader = torch.utils.data.DataLoader(\n",
    "    data_train, batch_size=N, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
    ")\n",
    "data = next(iter(sampling_train_loader))\n",
    "x_train = jnp.array(data[\"image\"])\n",
    "y_train = jnp.array(data[\"label\"])\n",
    "sampling_val_loader = torch.utils.data.DataLoader(\n",
    "    data_test, batch_size=N_test, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
    ")\n",
    "data = next(iter(sampling_val_loader))\n",
    "x_val = jnp.array(data[\"image\"])\n",
    "y_val = jnp.array(data[\"label\"])\n",
    "\n",
    "sample_key = jax.random.PRNGKey(0)\n",
    "n_posterior_samples = 200\n",
    "num_iterations = 1\n",
    "n_sample_batch_size = 1\n",
    "n_sample_batches = N // n_sample_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sampling.exact_ggn import exact_ggn_laplace\n",
    "from src.sampling.laplace_ode import ode_ggn\n",
    "from src.sampling.lanczos_diffusion import lanczos_diffusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_fn = lambda params, x: model.apply(params, x[None, ...])[0]\n",
    "ggn = calculate_exact_ggn(cross_entropy_loss, _model_fn, params, x_train, y_train, n_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals, eigvecs = jnp.linalg.eigh(ggn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "rank = 100\n",
    "\n",
    "def ggn_lr_vp(v):\n",
    "    return eigvecs[:,-rank:] @ jnp.diag(1/jnp.sqrt(eigvals[-rank:]+ alpha)) @ v\n",
    "\n",
    "def ggn_vp(v):\n",
    "    return eigvecs @ jnp.diag(1/jnp.sqrt(eigvals + alpha)) @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_posterior_samples = 20\n",
    "D = compute_num_params(params)\n",
    "sample_key = jax.random.PRNGKey(0)\n",
    "eps = jax.random.normal(sample_key, (n_posterior_samples, D))\n",
    "p0_flat, unravel_func_p = flatten_util.ravel_pytree(params)\n",
    "var = 0.1\n",
    "def get_posteriors(single_eps):\n",
    "    lr_sample = unravel_func_p(ggn_lr_vp(single_eps[:rank]) + p0_flat)\n",
    "    posterior_sample = unravel_func_p(ggn_vp(single_eps) + p0_flat)\n",
    "    isotropic_sample = unravel_func_p(var * single_eps + p0_flat)\n",
    "    return lr_sample, posterior_sample, isotropic_sample\n",
    "lr_posterior_samples, posterior_samples, isotropic_posterior_samples = jax.vmap(get_posteriors)(eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = sample_predictive(lr_posterior_samples, params, model, x_val, False, \"Pytree\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10000, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.5697, dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(params, model, x_val, y_val)/x_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_preds(preds, batch_y):\n",
    "    return jnp.sum(preds.argmax(axis=-1) == batch_y.argmax(axis=-1))\n",
    "accuracies = jax.vmap(accuracy_preds, in_axes=(0,None))(predictive, y_val)\n",
    "accuracies /= x_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.42203003, dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.4587, 0.4528, 0.3439, 0.4519, 0.4128, 0.3548, 0.424 , 0.4277,\n",
       "       0.44  , 0.4593, 0.4331, 0.4499, 0.4245, 0.4693, 0.4319, 0.4254,\n",
       "       0.3391, 0.4124, 0.4254, 0.4037], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lanczos Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sampling.low_rank import lanczos_tridiag\n",
    "from typing import Callable, Literal, Optional\n",
    "from functools import partial\n",
    "\n",
    "def get_gvp_fun(\n",
    "    model_fn: Callable,\n",
    "    loss_fn: Callable,\n",
    "    params,\n",
    "    x,\n",
    "    y\n",
    "  ) -> Callable:\n",
    "\n",
    "  def gvp(eps):\n",
    "    def scan_fun(carry, batch):\n",
    "      x_, y_ = batch\n",
    "      fn = lambda p: model_fn(p,x_[None,:])\n",
    "      loss_fn_ = lambda preds: loss_fn(preds, y_)\n",
    "      out, Je = jax.jvp(fn, (params,), (eps,))\n",
    "      _, HJe = jax.jvp(jax.jacrev(loss_fn_, argnums=0), (out,), (Je,))\n",
    "      _, vjp_fn = jax.vjp(fn, params)\n",
    "      value = vjp_fn(HJe)[0]\n",
    "      return jax.tree_map(lambda c, v: c + v, carry, value), None\n",
    "    init_value = jax.tree_map(lambda x: jnp.zeros_like(x), params)\n",
    "    return jax.lax.scan(scan_fun, init_value, (x, y))[0]\n",
    "  p0_flat, unravel_func_p = jax.flatten_util.ravel_pytree(params)\n",
    "  def matvec(v_like_params):\n",
    "    p_unravelled = unravel_func_p(v_like_params)\n",
    "    ggn_vp = gvp(p_unravelled)\n",
    "    f_eval, _ = jax.flatten_util.ravel_pytree(ggn_vp)\n",
    "    return f_eval\n",
    "  return matvec\n",
    "\n",
    "gvp = get_gvp_fun(model.apply, mse_loss, params, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 20\n",
    "n_samples = 50\n",
    "alpha = 10.0\n",
    "rank = 7\n",
    "eps = jax.random.normal(sample_key, (n_samples, n_steps, rank))\n",
    "p0_flat, unravel_func_p = jax.flatten_util.ravel_pytree(params)\n",
    "# rank = 100\n",
    "# alpha = 0.1\n",
    "v0 = jnp.ones(n_params)*5\n",
    "\n",
    "@jax.jit\n",
    "def rw_nonker(single_eps_path):\n",
    "    params_ = p0_flat\n",
    "    posterior_list = [params]\n",
    "    def body_fun(n, res):\n",
    "        gvp = get_gvp_fun(model.apply, mse_loss, unravel_func_p(res), x_train, y_train)\n",
    "        _, eigvecs = lanczos_tridiag(gvp, v0, rank - 1)\n",
    "        # ggn = calculate_exact_ggn(mse_loss, _model_fn, unravel_func_p(res), x_train, y_train, D)\n",
    "        # _, eigvecs = jnp.linalg.eigh(ggn)\n",
    "        lr_sample = 1/jnp.sqrt(alpha) * eigvecs @ single_eps_path[n]\n",
    "        params_ = res + 1/jnp.sqrt(n_steps) * lr_sample\n",
    "        return params_\n",
    "    v_ = jax.lax.fori_loop(0, n_steps - 1, body_fun, params_)\n",
    "    return unravel_func_p(v_)\n",
    "nonker_posterior_samples = jax.vmap(rw_nonker)(eps)#jax.lax.map(rw, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 20\n",
    "n_samples = 50\n",
    "alpha = 10.0\n",
    "rank = 10\n",
    "eps = jax.random.normal(sample_key, (n_samples, n_steps, n_params))\n",
    "p0_flat, unravel_func_p = jax.flatten_util.ravel_pytree(params)\n",
    "# rank = 100\n",
    "# alpha = 0.1\n",
    "v0 = jnp.ones(n_params)*5\n",
    "delta = 1.0\n",
    "\n",
    "@jax.jit\n",
    "def rw_ker(single_eps_path):\n",
    "    params_ = p0_flat\n",
    "    posterior_list = [params]\n",
    "    def body_fun(n, res):\n",
    "        gvp = get_gvp_fun(model.apply, mse_loss, unravel_func_p(res), x_train, y_train)\n",
    "        gvp_ = lambda v: gvp(v) + delta * v\n",
    "        eigvals, eigvecs = lanczos_tridiag(gvp_, v0, rank - 1)\n",
    "        # ggn = calculate_exact_ggn(mse_loss, _model_fn, unravel_func_p(res), x_train, y_train, D)\n",
    "        # _, eigvecs = jnp.linalg.eigh(ggn)\n",
    "        lr_sample = 1/jnp.sqrt(alpha) * 1/delta * (gvp_(single_eps_path[n]) - eigvecs @ jnp.diag(eigvals) @ eigvecs.T @ single_eps_path[n])\n",
    "        params_ = res + 1/jnp.sqrt(n_steps) * lr_sample\n",
    "        return params_\n",
    "    v_ = jax.lax.fori_loop(0, n_steps - 1, body_fun, params_)\n",
    "    return unravel_func_p(v_)\n",
    "ker_posterior_samples = jax.vmap(rw_ker)(eps)#jax.lax.map(rw, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 20\n",
    "n_samples = 50\n",
    "alpha = 10.0\n",
    "rank = 50\n",
    "nonker_posterior_samples = lanczos_diffusion(cross_entropy_loss, model.apply, params,n_steps,n_samples,alpha,sample_key,n_params,rank,x_train,y_train,1.0,\"non-kernel-eigvals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6318\n",
      "0.40998\n"
     ]
    }
   ],
   "source": [
    "def accuracy_preds(preds, batch_y):\n",
    "    return jnp.sum(preds.argmax(axis=-1) == batch_y.argmax(axis=-1))\n",
    "predictive_nonker = sample_predictive(nonker_posterior_samples, params, model, x_val, False, \"Pytree\")\n",
    "print(accuracy(params, model, x_val, y_val)/x_val.shape[0])\n",
    "nonker_accuracies = jax.vmap(accuracy_preds, in_axes=(0,None))(predictive_nonker, y_val)\n",
    "nonker_accuracies /= x_val.shape[0]\n",
    "print(jnp.mean(nonker_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_ker = sample_predictive(ker_posterior_samples, params, model, x_val, False, \"Pytree\")\n",
    "predictive_nonker = sample_predictive(nonker_posterior_samples, params, model, x_val, False, \"Pytree\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.6318, dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(params, model, x_val, y_val)/x_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099659994\n",
      "0.40998\n"
     ]
    }
   ],
   "source": [
    "ker_accuracies = jax.vmap(accuracy_preds, in_axes=(0,None))(predictive_ker, y_val)\n",
    "ker_accuracies /= x_val.shape[0]\n",
    "nonker_accuracies = jax.vmap(accuracy_preds, in_axes=(0,None))(predictive_nonker, y_val)\n",
    "nonker_accuracies /= x_val.shape[0]\n",
    "print(jnp.mean(ker_accuracies))\n",
    "print(jnp.mean(nonker_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
