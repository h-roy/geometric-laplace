{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import matfree\n",
    "import tree_math as tm\n",
    "from flax import linen as nn\n",
    "from jax import nn as jnn\n",
    "from jax import numpy as jnp\n",
    "from jax import random, jit\n",
    "import pickle\n",
    "from src.losses import mse_loss\n",
    "from src.helper import calculate_exact_ggn, tree_random_normal_like\n",
    "from src.sampling.predictive_samplers import sample_predictive, sample_hessian_predictive\n",
    "from jax import flatten_util\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.torch_datasets import MNIST, numpy_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOD dtasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets import (\n",
    "    get_rotated_mnist_loaders,\n",
    "    get_rotated_fmnist_loaders,\n",
    "    get_rotated_cifar_loaders,\n",
    "    load_corrupted_cifar10,\n",
    "    load_corrupted_cifar10_per_type,\n",
    "    get_mnist_ood_loaders,\n",
    "    get_cifar10_ood_loaders,\n",
    "    get_cifar10_train_set,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 30#1000\n",
    "classes_train = [0,1,2,3,4,5,6,7,8,9]\n",
    "n_classes = 10\n",
    "batch_size = 20#256\n",
    "test_batch_size = 256\n",
    "\n",
    "data_train = MNIST(path_root= \"/work3/hroy/data/\",\n",
    "            train=True, n_samples=train_samples if train_samples > 0 else None, cls=classes_train\n",
    "        )\n",
    "data_test = MNIST(path_root = \"/work3/hroy/data/\", train=False, cls=classes_train)\n",
    "\n",
    "if train_samples > 0:\n",
    "    N = train_samples * n_classes\n",
    "else:\n",
    "    N = len(data_train)\n",
    "N_test = len(data_test)\n",
    "if test_batch_size > 0:\n",
    "    test_batch_size = test_batch_size\n",
    "else:\n",
    "    test_batch_size = len(data_test)\n",
    "\n",
    "n_test_batches = int(N_test / test_batch_size)\n",
    "n_batches = int(N / batch_size)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    data_train, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    data_test, batch_size=test_batch_size, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    output_dim: int = 10\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if len(x.shape) != 4:\n",
    "            x = jnp.expand_dims(x, 0)\n",
    "        x = jnp.transpose(x, (0, 2, 3, 1))\n",
    "        x = nn.Conv(features=4, kernel_size=(3, 3), strides=(2, 2), padding=1)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.Conv(features=4, kernel_size=(3, 3), strides=(2, 2), padding=1)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        return nn.Dense(features=self.output_dim)(x)\n",
    "\n",
    "def compute_num_params(pytree):\n",
    "    return sum(x.size if hasattr(x, \"size\") else 0 for x in jax.tree_util.tree_leaves(pytree))\n",
    "\n",
    "\n",
    "model = ConvNet()\n",
    "batch = next(iter(train_loader))\n",
    "x_init, y_init = batch[\"image\"], batch[\"label\"]\n",
    "output_dim = y_init.shape[-1]\n",
    "key, split_key = random.split(jax.random.PRNGKey(0))\n",
    "params = model.init(key, x_init)\n",
    "alpha = 1.\n",
    "optim = optax.chain(\n",
    "        optax.clip(1.),\n",
    "        getattr(optax, \"adam\")(1e-2),\n",
    "    )\n",
    "opt_state = optim.init(params)\n",
    "n_params = compute_num_params(params)\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(preds, y, rho=1.0):\n",
    "    \"\"\"\n",
    "    preds: (n_samples, n_classes) (logits)\n",
    "    y: (n_samples, n_classes) (one-hot labels)\n",
    "    \"\"\"\n",
    "    preds = preds * rho\n",
    "    preds = jax.nn.log_softmax(preds, axis=-1)\n",
    "    return -jnp.sum(jnp.sum(preds * y, axis=-1))\n",
    "\n",
    "def accuracy(params, model, batch_x, batch_y):\n",
    "    preds = model.apply(params, batch_x)\n",
    "    return jnp.sum(preds.argmax(axis=-1) == batch_y.argmax(axis=-1))\n",
    "\n",
    "\n",
    "def map_loss(\n",
    "    params,\n",
    "    model,\n",
    "    x_batch,\n",
    "    y_batch,\n",
    "    alpha,\n",
    "    n_params: int,\n",
    "    N_datapoints_max: int,\n",
    "):\n",
    "    # define dict for logging purposes\n",
    "    B = x_batch.shape[0]\n",
    "    O = y_batch.shape[-1]\n",
    "    D = n_params\n",
    "    N = N_datapoints_max\n",
    "\n",
    "    # hessian_scaler = 1\n",
    "\n",
    "    vparams = tm.Vector(params)\n",
    "\n",
    "    rho = 1.\n",
    "    nll = lambda x, y, rho: 1/B * cross_entropy_loss(x, y, rho)\n",
    "\n",
    "    y_pred = model.apply(params, x_batch)\n",
    "\n",
    "    loglike_loss = nll(y_pred, y_batch, rho) #* hessian_scaler\n",
    "\n",
    "    log_prior_term = -D / 2 * jnp.log(2 * jnp.pi) - (1 / 2) * alpha * (vparams @ vparams) + D / 2 * jnp.log(alpha)\n",
    "    # log_det_term = 0\n",
    "    loss = loglike_loss - 0. * log_prior_term\n",
    "\n",
    "    return loss\n",
    "\n",
    "def make_step(params, alpha, opt_state, x, y):\n",
    "    grad_fn = jax.value_and_grad(map_loss, argnums=0, has_aux=False)\n",
    "    loss, grads = grad_fn(params, model, x, y, alpha, n_params, N)\n",
    "    param_updates, opt_state = optim.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, param_updates)\n",
    "    return loss, params, opt_state\n",
    "\n",
    "jit_make_step = jit(make_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1, loss=34.398, , accuracy=0.19, alpha=1.00, time=1.365s\n",
      "epoch=2, loss=32.283, , accuracy=0.33, alpha=1.00, time=0.249s\n",
      "epoch=3, loss=29.460, , accuracy=0.43, alpha=1.00, time=0.251s\n",
      "epoch=4, loss=26.536, , accuracy=0.51, alpha=1.00, time=0.242s\n",
      "epoch=5, loss=23.831, , accuracy=0.57, alpha=1.00, time=0.241s\n",
      "epoch=6, loss=21.496, , accuracy=0.59, alpha=1.00, time=0.242s\n",
      "epoch=7, loss=19.704, , accuracy=0.64, alpha=1.00, time=0.256s\n",
      "epoch=8, loss=18.153, , accuracy=0.65, alpha=1.00, time=0.242s\n",
      "epoch=9, loss=16.757, , accuracy=0.69, alpha=1.00, time=0.241s\n",
      "epoch=10, loss=15.860, , accuracy=0.70, alpha=1.00, time=0.241s\n",
      "epoch=11, loss=14.923, , accuracy=0.74, alpha=1.00, time=0.247s\n",
      "epoch=12, loss=14.055, , accuracy=0.75, alpha=1.00, time=0.239s\n",
      "epoch=13, loss=13.380, , accuracy=0.77, alpha=1.00, time=0.246s\n",
      "epoch=14, loss=12.759, , accuracy=0.76, alpha=1.00, time=0.271s\n",
      "epoch=15, loss=12.163, , accuracy=0.79, alpha=1.00, time=0.239s\n",
      "epoch=16, loss=11.796, , accuracy=0.79, alpha=1.00, time=0.240s\n",
      "epoch=17, loss=11.260, , accuracy=0.80, alpha=1.00, time=0.240s\n",
      "epoch=18, loss=10.825, , accuracy=0.83, alpha=1.00, time=0.242s\n",
      "epoch=19, loss=10.252, , accuracy=0.83, alpha=1.00, time=0.237s\n",
      "epoch=20, loss=9.841, , accuracy=0.83, alpha=1.00, time=0.236s\n",
      "epoch=21, loss=9.586, , accuracy=0.85, alpha=1.00, time=0.243s\n",
      "epoch=22, loss=9.176, , accuracy=0.83, alpha=1.00, time=0.245s\n",
      "epoch=23, loss=8.795, , accuracy=0.86, alpha=1.00, time=0.238s\n",
      "epoch=24, loss=8.617, , accuracy=0.86, alpha=1.00, time=0.239s\n",
      "epoch=25, loss=8.291, , accuracy=0.86, alpha=1.00, time=0.237s\n",
      "epoch=26, loss=8.100, , accuracy=0.87, alpha=1.00, time=0.238s\n",
      "epoch=27, loss=8.066, , accuracy=0.86, alpha=1.00, time=0.239s\n",
      "epoch=28, loss=7.741, , accuracy=0.88, alpha=1.00, time=0.248s\n",
      "epoch=29, loss=7.432, , accuracy=0.88, alpha=1.00, time=0.245s\n",
      "epoch=30, loss=7.285, , accuracy=0.87, alpha=1.00, time=0.242s\n",
      "epoch=31, loss=7.016, , accuracy=0.89, alpha=1.00, time=0.235s\n",
      "epoch=32, loss=6.828, , accuracy=0.87, alpha=1.00, time=0.241s\n",
      "epoch=33, loss=6.672, , accuracy=0.88, alpha=1.00, time=0.242s\n",
      "epoch=34, loss=6.659, , accuracy=0.89, alpha=1.00, time=0.237s\n",
      "epoch=35, loss=6.395, , accuracy=0.88, alpha=1.00, time=0.244s\n",
      "epoch=36, loss=6.138, , accuracy=0.90, alpha=1.00, time=0.235s\n",
      "epoch=37, loss=6.117, , accuracy=0.89, alpha=1.00, time=0.235s\n",
      "epoch=38, loss=6.118, , accuracy=0.89, alpha=1.00, time=0.235s\n",
      "epoch=39, loss=5.861, , accuracy=0.90, alpha=1.00, time=0.235s\n",
      "epoch=40, loss=5.715, , accuracy=0.89, alpha=1.00, time=0.235s\n",
      "epoch=41, loss=5.853, , accuracy=0.89, alpha=1.00, time=0.235s\n",
      "epoch=42, loss=5.539, , accuracy=0.91, alpha=1.00, time=0.235s\n",
      "epoch=43, loss=5.459, , accuracy=0.92, alpha=1.00, time=0.235s\n",
      "epoch=44, loss=5.139, , accuracy=0.92, alpha=1.00, time=0.236s\n",
      "epoch=45, loss=5.294, , accuracy=0.92, alpha=1.00, time=0.236s\n",
      "epoch=46, loss=4.991, , accuracy=0.93, alpha=1.00, time=0.235s\n",
      "epoch=47, loss=5.090, , accuracy=0.92, alpha=1.00, time=0.242s\n",
      "epoch=48, loss=4.826, , accuracy=0.93, alpha=1.00, time=0.236s\n",
      "epoch=49, loss=4.726, , accuracy=0.94, alpha=1.00, time=0.236s\n",
      "epoch=50, loss=4.645, , accuracy=0.94, alpha=1.00, time=0.236s\n",
      "epoch=51, loss=4.659, , accuracy=0.93, alpha=1.00, time=0.236s\n",
      "epoch=52, loss=4.466, , accuracy=0.94, alpha=1.00, time=0.236s\n",
      "epoch=53, loss=4.352, , accuracy=0.94, alpha=1.00, time=0.236s\n",
      "epoch=54, loss=4.246, , accuracy=0.94, alpha=1.00, time=0.237s\n",
      "epoch=55, loss=4.288, , accuracy=0.94, alpha=1.00, time=0.236s\n",
      "epoch=56, loss=4.050, , accuracy=0.96, alpha=1.00, time=0.236s\n",
      "epoch=57, loss=4.103, , accuracy=0.95, alpha=1.00, time=0.236s\n",
      "epoch=58, loss=4.090, , accuracy=0.95, alpha=1.00, time=0.236s\n",
      "epoch=59, loss=3.969, , accuracy=0.95, alpha=1.00, time=0.236s\n",
      "epoch=60, loss=3.811, , accuracy=0.95, alpha=1.00, time=0.236s\n",
      "epoch=61, loss=3.881, , accuracy=0.95, alpha=1.00, time=0.236s\n",
      "epoch=62, loss=3.714, , accuracy=0.96, alpha=1.00, time=0.236s\n",
      "epoch=63, loss=3.597, , accuracy=0.96, alpha=1.00, time=0.236s\n",
      "epoch=64, loss=3.457, , accuracy=0.96, alpha=1.00, time=0.236s\n",
      "epoch=65, loss=3.488, , accuracy=0.96, alpha=1.00, time=0.236s\n",
      "epoch=66, loss=3.373, , accuracy=0.96, alpha=1.00, time=0.242s\n",
      "epoch=67, loss=3.317, , accuracy=0.96, alpha=1.00, time=0.235s\n",
      "epoch=68, loss=3.333, , accuracy=0.96, alpha=1.00, time=0.243s\n",
      "epoch=69, loss=3.256, , accuracy=0.96, alpha=1.00, time=0.234s\n",
      "epoch=70, loss=3.227, , accuracy=0.97, alpha=1.00, time=0.235s\n",
      "epoch=71, loss=3.170, , accuracy=0.97, alpha=1.00, time=0.234s\n",
      "epoch=72, loss=3.136, , accuracy=0.97, alpha=1.00, time=0.234s\n",
      "epoch=73, loss=3.260, , accuracy=0.95, alpha=1.00, time=0.234s\n",
      "epoch=74, loss=3.296, , accuracy=0.96, alpha=1.00, time=0.241s\n",
      "epoch=75, loss=3.096, , accuracy=0.97, alpha=1.00, time=0.233s\n",
      "epoch=76, loss=2.872, , accuracy=0.97, alpha=1.00, time=0.233s\n",
      "epoch=77, loss=3.030, , accuracy=0.97, alpha=1.00, time=0.242s\n",
      "epoch=78, loss=2.981, , accuracy=0.97, alpha=1.00, time=0.234s\n",
      "epoch=79, loss=2.959, , accuracy=0.97, alpha=1.00, time=0.236s\n",
      "epoch=80, loss=2.783, , accuracy=0.97, alpha=1.00, time=0.234s\n",
      "epoch=81, loss=2.693, , accuracy=0.97, alpha=1.00, time=0.234s\n",
      "epoch=82, loss=2.689, , accuracy=0.98, alpha=1.00, time=0.235s\n",
      "epoch=83, loss=2.718, , accuracy=0.97, alpha=1.00, time=0.234s\n",
      "epoch=84, loss=2.637, , accuracy=0.98, alpha=1.00, time=0.234s\n",
      "epoch=85, loss=2.694, , accuracy=0.97, alpha=1.00, time=0.234s\n",
      "epoch=86, loss=2.557, , accuracy=0.98, alpha=1.00, time=0.243s\n",
      "epoch=87, loss=2.716, , accuracy=0.97, alpha=1.00, time=0.235s\n",
      "epoch=88, loss=2.419, , accuracy=0.98, alpha=1.00, time=0.235s\n",
      "epoch=89, loss=2.359, , accuracy=0.98, alpha=1.00, time=0.235s\n",
      "epoch=90, loss=2.337, , accuracy=0.98, alpha=1.00, time=0.235s\n",
      "epoch=91, loss=2.298, , accuracy=0.98, alpha=1.00, time=0.235s\n",
      "epoch=92, loss=2.381, , accuracy=0.99, alpha=1.00, time=0.235s\n",
      "epoch=93, loss=2.388, , accuracy=0.98, alpha=1.00, time=0.241s\n",
      "epoch=94, loss=2.252, , accuracy=0.98, alpha=1.00, time=0.234s\n",
      "epoch=95, loss=2.187, , accuracy=0.98, alpha=1.00, time=0.234s\n",
      "epoch=96, loss=2.231, , accuracy=0.98, alpha=1.00, time=0.234s\n",
      "epoch=97, loss=2.212, , accuracy=0.98, alpha=1.00, time=0.235s\n",
      "epoch=98, loss=2.042, , accuracy=0.99, alpha=1.00, time=0.234s\n",
      "epoch=99, loss=2.154, , accuracy=0.98, alpha=1.00, time=0.234s\n",
      "epoch=100, loss=2.173, , accuracy=0.99, alpha=1.00, time=0.240s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    start_time = time.time()\n",
    "    for _, batch in zip(range(n_batches), train_loader):\n",
    "        X = batch[\"image\"]\n",
    "        y = batch[\"label\"]\n",
    "        B = X.shape[0]\n",
    "        train_key, split_key = random.split(split_key)\n",
    "\n",
    "        loss, params, opt_state = jit_make_step(params, alpha, opt_state, X, y)\n",
    "        loss = loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        epoch_accuracy += accuracy(params, model, X, y).item()\n",
    "\n",
    "    epoch_accuracy /= (n_batches * B)\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(\n",
    "        f\"epoch={epoch}, loss={epoch_loss:.3f}, , accuracy={epoch_accuracy:.2f}, alpha={alpha:.2f}, time={epoch_time:.3f}s\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_train_loader = torch.utils.data.DataLoader(\n",
    "    data_train, batch_size=N, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
    ")\n",
    "data = next(iter(sampling_train_loader))\n",
    "x_train = jnp.array(data[\"image\"])\n",
    "y_train = jnp.array(data[\"label\"])\n",
    "sampling_val_loader = torch.utils.data.DataLoader(\n",
    "    data_test, batch_size=N_test, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
    ")\n",
    "data = next(iter(sampling_val_loader))\n",
    "x_val = jnp.array(data[\"image\"])\n",
    "y_val = jnp.array(data[\"label\"])\n",
    "\n",
    "sample_key = jax.random.PRNGKey(0)\n",
    "n_posterior_samples = 200\n",
    "num_iterations = 1\n",
    "n_sample_batch_size = 1\n",
    "n_sample_batches = N // n_sample_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sampling.exact_ggn import exact_ggn_laplace\n",
    "from src.sampling.laplace_ode import ode_ggn\n",
    "from src.sampling.lanczos_diffusion import lanczos_diffusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_fn = lambda params, x: model.apply(params, x[None, ...])[0]\n",
    "ggn = calculate_exact_ggn(cross_entropy_loss, _model_fn, params, x_train, y_train, n_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals, eigvecs = jnp.linalg.eigh(ggn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "rank = 100\n",
    "\n",
    "def ggn_lr_vp(v):\n",
    "    return eigvecs[:,-rank:] @ jnp.diag(1/jnp.sqrt(eigvals[-rank:]+ alpha)) @ v\n",
    "\n",
    "def ggn_vp(v):\n",
    "    return eigvecs @ jnp.diag(1/jnp.sqrt(eigvals + alpha)) @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_posterior_samples = 20\n",
    "D = compute_num_params(params)\n",
    "sample_key = jax.random.PRNGKey(0)\n",
    "eps = jax.random.normal(sample_key, (n_posterior_samples, D))\n",
    "p0_flat, unravel_func_p = flatten_util.ravel_pytree(params)\n",
    "var = 0.1\n",
    "def get_posteriors(single_eps):\n",
    "    lr_sample = unravel_func_p(ggn_lr_vp(single_eps[:rank]) + p0_flat)\n",
    "    posterior_sample = unravel_func_p(ggn_vp(single_eps) + p0_flat)\n",
    "    isotropic_sample = unravel_func_p(var * single_eps + p0_flat)\n",
    "    return lr_sample, posterior_sample, isotropic_sample\n",
    "lr_posterior_samples, posterior_samples, isotropic_posterior_samples = jax.vmap(get_posteriors)(eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = sample_predictive(lr_posterior_samples, params, model, x_val, False, \"Pytree\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10000, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.6786, dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(params, model, x_val, y_val)/x_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_preds(preds, batch_y):\n",
    "    return jnp.sum(preds.argmax(axis=-1) == batch_y.argmax(axis=-1))\n",
    "accuracies = jax.vmap(accuracy_preds, in_axes=(0,None))(predictive, y_val)\n",
    "accuracies /= x_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.64703, dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.mean(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lanczos Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sampling.low_rank import lanczos_tridiag\n",
    "from typing import Callable, Literal, Optional\n",
    "from functools import partial\n",
    "\n",
    "def get_gvp_fun(\n",
    "    model_fn: Callable,\n",
    "    loss_fn: Callable,\n",
    "    params,\n",
    "    x,\n",
    "    y\n",
    "  ) -> Callable:\n",
    "\n",
    "  def gvp(eps):\n",
    "    def scan_fun(carry, batch):\n",
    "      x_, y_ = batch\n",
    "      fn = lambda p: model_fn(p,x_[None,:])\n",
    "      loss_fn_ = lambda preds: loss_fn(preds, y_)\n",
    "      out, Je = jax.jvp(fn, (params,), (eps,))\n",
    "      _, HJe = jax.jvp(jax.jacrev(loss_fn_, argnums=0), (out,), (Je,))\n",
    "      _, vjp_fn = jax.vjp(fn, params)\n",
    "      value = vjp_fn(HJe)[0]\n",
    "      return jax.tree_map(lambda c, v: c + v, carry, value), None\n",
    "    init_value = jax.tree_map(lambda x: jnp.zeros_like(x), params)\n",
    "    return jax.lax.scan(scan_fun, init_value, (x, y))[0]\n",
    "  p0_flat, unravel_func_p = jax.flatten_util.ravel_pytree(params)\n",
    "  def matvec(v_like_params):\n",
    "    p_unravelled = unravel_func_p(v_like_params)\n",
    "    ggn_vp = gvp(p_unravelled)\n",
    "    f_eval, _ = jax.flatten_util.ravel_pytree(ggn_vp)\n",
    "    return f_eval\n",
    "  return matvec\n",
    "\n",
    "gvp = get_gvp_fun(model.apply, mse_loss, params, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 20\n",
    "n_samples = 50\n",
    "alpha = 10.0\n",
    "rank = 7\n",
    "eps = jax.random.normal(sample_key, (n_samples, n_steps, rank))\n",
    "p0_flat, unravel_func_p = jax.flatten_util.ravel_pytree(params)\n",
    "# rank = 100\n",
    "# alpha = 0.1\n",
    "v0 = jnp.ones(n_params)*5\n",
    "\n",
    "@jax.jit\n",
    "def rw_nonker(single_eps_path):\n",
    "    params_ = p0_flat\n",
    "    posterior_list = [params]\n",
    "    def body_fun(n, res):\n",
    "        gvp = get_gvp_fun(model.apply, mse_loss, unravel_func_p(res), x_train, y_train)\n",
    "        _, eigvecs = lanczos_tridiag(gvp, v0, rank - 1)\n",
    "        # ggn = calculate_exact_ggn(mse_loss, _model_fn, unravel_func_p(res), x_train, y_train, D)\n",
    "        # _, eigvecs = jnp.linalg.eigh(ggn)\n",
    "        lr_sample = 1/jnp.sqrt(alpha) * eigvecs @ single_eps_path[n]\n",
    "        params_ = res + 1/jnp.sqrt(n_steps) * lr_sample\n",
    "        return params_\n",
    "    v_ = jax.lax.fori_loop(0, n_steps - 1, body_fun, params_)\n",
    "    return unravel_func_p(v_)\n",
    "nonker_posterior_samples = jax.vmap(rw_nonker)(eps)#jax.lax.map(rw, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 20\n",
    "n_samples = 50\n",
    "alpha = 10.0\n",
    "rank = 10\n",
    "eps = jax.random.normal(sample_key, (n_samples, n_steps, n_params))\n",
    "p0_flat, unravel_func_p = jax.flatten_util.ravel_pytree(params)\n",
    "# rank = 100\n",
    "# alpha = 0.1\n",
    "v0 = jnp.ones(n_params)*5\n",
    "delta = 1.0\n",
    "\n",
    "@jax.jit\n",
    "def rw_ker(single_eps_path):\n",
    "    params_ = p0_flat\n",
    "    posterior_list = [params]\n",
    "    def body_fun(n, res):\n",
    "        gvp = get_gvp_fun(model.apply, mse_loss, unravel_func_p(res), x_train, y_train)\n",
    "        gvp_ = lambda v: gvp(v) + delta * v\n",
    "        eigvals, eigvecs = lanczos_tridiag(gvp_, v0, rank - 1)\n",
    "        # ggn = calculate_exact_ggn(mse_loss, _model_fn, unravel_func_p(res), x_train, y_train, D)\n",
    "        # _, eigvecs = jnp.linalg.eigh(ggn)\n",
    "        lr_sample = 1/jnp.sqrt(alpha) * 1/delta * (gvp_(single_eps_path[n]) - eigvecs @ jnp.diag(eigvals) @ eigvecs.T @ single_eps_path[n])\n",
    "        params_ = res + 1/jnp.sqrt(n_steps) * lr_sample\n",
    "        return params_\n",
    "    v_ = jax.lax.fori_loop(0, n_steps - 1, body_fun, params_)\n",
    "    return unravel_func_p(v_)\n",
    "ker_posterior_samples = jax.vmap(rw_ker)(eps)#jax.lax.map(rw, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 20\n",
    "n_samples = 50\n",
    "alpha = 10.0\n",
    "rank = 50\n",
    "nonker_posterior_samples = lanczos_diffusion(cross_entropy_loss, model.apply, params,n_steps,n_samples,alpha,sample_key,n_params,rank,x_train,y_train,1.0,\"non-kernel-eigvals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6318\n",
      "0.40998\n"
     ]
    }
   ],
   "source": [
    "def accuracy_preds(preds, batch_y):\n",
    "    return jnp.sum(preds.argmax(axis=-1) == batch_y.argmax(axis=-1))\n",
    "predictive_nonker = sample_predictive(nonker_posterior_samples, params, model, x_val, False, \"Pytree\")\n",
    "print(accuracy(params, model, x_val, y_val)/x_val.shape[0])\n",
    "nonker_accuracies = jax.vmap(accuracy_preds, in_axes=(0,None))(predictive_nonker, y_val)\n",
    "nonker_accuracies /= x_val.shape[0]\n",
    "print(jnp.mean(nonker_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_ker = sample_predictive(ker_posterior_samples, params, model, x_val, False, \"Pytree\")\n",
    "predictive_nonker = sample_predictive(nonker_posterior_samples, params, model, x_val, False, \"Pytree\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.6318, dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(params, model, x_val, y_val)/x_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099659994\n",
      "0.40998\n"
     ]
    }
   ],
   "source": [
    "ker_accuracies = jax.vmap(accuracy_preds, in_axes=(0,None))(predictive_ker, y_val)\n",
    "ker_accuracies /= x_val.shape[0]\n",
    "nonker_accuracies = jax.vmap(accuracy_preds, in_axes=(0,None))(predictive_nonker, y_val)\n",
    "nonker_accuracies /= x_val.shape[0]\n",
    "print(jnp.mean(ker_accuracies))\n",
    "print(jnp.mean(nonker_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
